#!/usr/bin/env python3
import os
import re
import time
import torch
import logging
import argparse
import numpy as np
from tqdm import tqdm
from datetime import datetime
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    get_scheduler
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("finetuning.log")
    ]
)
logger = logging.getLogger(__name__)

class ProgressCallback(torch.utils.data.Dataset):
    """Custom dataset wrapper to show progress during processing."""
    def __init__(self, dataset, desc="Processing"):
        self.dataset = dataset
        self.desc = desc
        self.pbar = tqdm(total=len(dataset), desc=desc)
        self.processed = 0
        self.start_time = time.time()
    
    def __getitem__(self, idx):
        item = self.dataset[idx]
        self.processed += 1
        
        # Update progress bar every 10 items
        if self.processed % 10 == 0:
            self.pbar.update(10)
            items_per_second = self.processed / (time.time() - self.start_time)
            self.pbar.set_postfix({
                'items/sec': f'{items_per_second:.2f}', 
                'remaining': f'{(len(self.dataset) - self.processed) / max(1, items_per_second):.2f}s'
            })
            
        return item
    
    def __len__(self):
        return len(self.dataset)

class CustomTrainer(Trainer):
    """Custom trainer with additional reporting."""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_time = time.time()
        self.log_history = []
        
    def log(self, logs):
        """Override log method to add more metrics."""
        logs = logs.copy()
        
        elapsed = time.time() - self.start_time
        logs["elapsed"] = elapsed
        
        if "loss" in logs and hasattr(self, "state") and self.state.global_step > 0:
            time_per_step = elapsed / self.state.global_step
            remaining_steps = self.state.max_steps - self.state.global_step
            logs["time_per_step"] = time_per_step
            logs["eta"] = time_per_step * remaining_steps
            
        if torch.cuda.is_available():
            logs["gpu_mem_allocated"] = torch.cuda.memory_allocated() / 1024**2
            logs["gpu_mem_reserved"] = torch.cuda.memory_reserved() / 1024**2
            
        self.log_history.append(logs)
        
        super().log(logs)

def parse_business_reviews(file_path):
    """Parse the business reviews file into a list of dictionaries."""
    logger.info(f"Reading business reviews from {file_path}")
    start_time = time.time()
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    reviews_raw = content.split('---')
    logger.info(f"Found {len(reviews_raw)} raw review entries")
    
    reviews = []
    pattern = re.compile(r'Business Name: (.*?)\nCategory: (.*?)\nReview: (.*?)(?:\nRating: (\d+))?(?:\nResponse: (.*?))?(?:\nAddress: (.*?))?(?:\nRating: ([\d\.]+))?(?:\nGMAP ID: (.*?))?(?:\nLatitude: ([\d\.-]+))?(?:\nLongitude: ([\d\.-]+))?(?:\nDescription: (.*?))?(?:\nAverage Rating: ([\d\.]+))?(?:\nPrice: (.*?))?(?:\nHours: (.*?))?(?:\n|$)', re.DOTALL)
    
    for i, review_text in enumerate(tqdm(reviews_raw, desc="Parsing reviews")):
        if not review_text.strip():
            continue
            
        match = pattern.search(review_text.strip())
        if match:
            business_name = match.group(1) or ""
            category = match.group(2) or ""
            review = match.group(3) or ""
            rating = match.group(4) or ""
            response = match.group(5) or ""
            address = match.group(6) or ""
            avg_rating = match.group(7) or ""
            description = match.group(11) or ""
            
            reviews.append({
                "business_name": business_name.strip(),
                "category": category.strip(),
                "review": review.strip(),
                "rating": rating.strip(),
                "response": "None" if response is None or response.strip() == "None" else response.strip(),
                "address": address.strip(),
                "avg_rating": avg_rating.strip(),
                "description": description.strip()
            })
        else:
            logger.warning(f"Failed to parse review {i+1}")
    
    elapsed = time.time() - start_time
    items_per_second = len(reviews) / elapsed
    logger.info(f"Parsed {len(reviews)} reviews in {elapsed:.2f}s ({items_per_second:.2f} reviews/second)")
    
    return reviews

def format_prompt(review_data):
    """Format review data into prompt format."""
    categories = review_data["category"].replace("'", "").replace("[", "").replace("]", "")
    
    prompt = f"""Business: {review_data["business_name"]}
Categories: {categories}
Description: {review_data["description"]}
Average Rating: {review_data["avg_rating"]}
Review: {review_data["review"]}
Rating: {review_data["rating"]}

Task: Generate a polite and helpful response to this customer review.

Response:"""
    
    if review_data["response"] and review_data["response"] != "None":
        prompt += f" {review_data['response']}"
    
    return prompt

def prepare_dataset(reviews, tokenizer, max_length=512):
    """Prepare dataset for fine-tuning."""
    logger.info("Preparing dataset for fine-tuning")
    start_time = time.time()
    
    formatted_data = []
    for review in tqdm(reviews, desc="Formatting prompts"):
        formatted_data.append({
            "text": format_prompt(review)
        })
    
    dataset = Dataset.from_list(formatted_data)
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )
    
    logger.info(f"Tokenizing {len(dataset)} examples (this may take a while)...")
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        desc="Tokenizing",
        remove_columns=["text"]
    )
    
    elapsed = time.time() - start_time
    items_per_second = len(dataset) / elapsed
    logger.info(f"Prepared dataset with {len(dataset)} examples in {elapsed:.2f}s ({items_per_second:.2f} examples/second)")
    
    return tokenized_dataset

def load_model_with_progress(model_name):
    """Load model with progress information."""
    logger.info(f"Loading model: {model_name}")
    start_time = time.time()
    
    print(f"Loading tokenizer... ", end="", flush=True)
    tokenizer_start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer_time = time.time() - tokenizer_start
    print(f"Done! ({tokenizer_time:.2f}s)")
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"Loading model... ", end="", flush=True)
    model_start = time.time()
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="auto"
    )
    model_time = time.time() - model_start
    print(f"Done! ({model_time:.2f}s)")
    
    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)
    
    elapsed = time.time() - start_time
    logger.info(f"Model loaded in {elapsed:.2f}s. Model size: {model_size_mb:.2f} MB")
    logger.info(f"- Tokenizer: {tokenizer_time:.2f}s")
    logger.info(f"- Model: {model_time:.2f}s")
    
    return model, tokenizer

def main():
    parser = argparse.ArgumentParser(description="Fine-tune Llama-3.2-1B on business reviews")
    parser.add_argument("--model_name", type=str, default="meta-llama/Llama-3.2-1B", help="Model name or path")
    parser.add_argument("--data_file", type=str, default="business_reviews.txt", help="Path to business reviews file")
    parser.add_argument("--output_dir", type=str, default="./finetuned-llama-business", help="Output directory for checkpoints")
    parser.add_argument("--num_epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for training")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4, help="Gradient accumulation steps")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="Learning rate")
    parser.add_argument("--max_length", type=int, default=512, help="Maximum sequence length")
    parser.add_argument("--logging_steps", type=int, default=10, help="Logging steps")
    parser.add_argument("--save_steps", type=int, default=500, help="Save steps")
    parser.add_argument("--save_total_limit", type=int, default=3, help="Save total limit")
    parser.add_argument("--warmup_steps", type=int, default=100, help="Warmup steps")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="Weight decay")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--fp16", action="store_true", help="Use FP16 training")
    parser.add_argument("--max_reviews", type=int, default=None, help="Maximum number of reviews to use (for testing)")
    args = parser.parse_args()
    
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    total_start_time = time.time()
    
    model, tokenizer = load_model_with_progress(args.model_name)
    
    reviews = parse_business_reviews(args.data_file)
    if args.max_reviews:
        logger.info(f"Using only {args.max_reviews} reviews for testing")
        reviews = reviews[:args.max_reviews]
    
    tokenized_dataset = prepare_dataset(reviews, tokenizer, args.max_length)
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Not using masked language modeling for causal language models
    )
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        adam_beta1=0.9,
        adam_beta2=0.999,
        adam_epsilon=1e-8,
        max_grad_norm=1.0,
        lr_scheduler_type="cosine",
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_strategy="steps",
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        fp16=args.fp16 or torch.backends.mps.is_available(),  # Use fp16 if available
        report_to="none",  # Disable wandb, etc.
        seed=args.seed,
        logging_dir=os.path.join(args.output_dir, "logs"),
    )
    
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    logger.info(f"Starting fine-tuning with {len(tokenized_dataset)} examples")
    logger.info(f"  Model: {args.model_name}")
    logger.info(f"  Output directory: {args.output_dir}")
    logger.info(f"  Number of epochs: {args.num_epochs}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Gradient accumulation steps: {args.gradient_accumulation_steps}")
    logger.info(f"  Effective batch size: {args.batch_size * args.gradient_accumulation_steps}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Warmup steps: {args.warmup_steps}")
    
    total_steps = (len(tokenized_dataset) // (args.batch_size * args.gradient_accumulation_steps)) * args.num_epochs
    logger.info(f"  Total training steps: {total_steps}")
    logger.info(f"  Expected training time: To be determined during training")
    
    logger.info("Starting training...")
    trainer.train()
    
    final_output_dir = os.path.join(args.output_dir, "final-model")
    os.makedirs(final_output_dir, exist_ok=True)
    logger.info(f"Saving final model to {final_output_dir}")
    
    model.save_pretrained(final_output_dir)
    tokenizer.save_pretrained(final_output_dir)
    
    total_time = time.time() - total_start_time
    logger.info(f"Training complete! Total time: {total_time:.2f}s ({total_time/3600:.2f}h)")
    
    logger.info(f"""
==================================================
Fine-tuning complete!
--------------------------------------------------
- Total training time: {total_time:.2f}s ({total_time/3600:.2f}h)
- Model saved to: {final_output_dir}
==================================================
    """)

if __name__ == "__main__":
    main()
